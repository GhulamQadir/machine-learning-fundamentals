{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045a17f5",
   "metadata": {},
   "source": [
    "# **Probability & Statistics Foundations for ML**\n",
    "\n",
    "\n",
    "\n",
    "## 1. Basic Statistics\n",
    "- **Mean** ‚Üí Average of numbers.  \n",
    "- **Median** ‚Üí Middle value when numbers are sorted.  \n",
    "- **Mode** ‚Üí Most frequent value.  \n",
    "- **Standard Deviation (SD)** ‚Üí Spread of data.  \n",
    "\n",
    "### Standard Deviation\n",
    "- Low SD ‚Üí data tightly packed.  \n",
    "- High SD ‚Üí data spread out.  \n",
    "\n",
    "**Example:**  \n",
    "- Group A: [150, 149, 151, 150, 150] ‚Üí tightly packed (low SD)  \n",
    "- Group B: [100, 200, 150, 180, 120] ‚Üí spread out (high SD)  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Probability\n",
    "Probability = Chances of an event occurring.  \n",
    "\n",
    "**Example:**  \n",
    "- 20 out of 100 people have heart disease.  \n",
    "- P(disease) = 20/100 = 0.20 = **20%**  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Conditional Probability\n",
    "Probability of one event given that another has occurred.  \n",
    "\n",
    "**Example:**  \n",
    "- P(disease | Chest Pain = Yes)  \n",
    "= Probability that a patient has heart disease given chest pain.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Covariance\n",
    "- Measures how two variables change together.\n",
    "- Positive ‚Üí both increase/decrease together.\n",
    "- Negative ‚Üí one increases, the other decreases.\n",
    "- Zero ‚Üí no relation.\n",
    "\n",
    "**Example 1:**<br>\n",
    "1. Height & Weight ‚Üí positive covariance (taller ‚Üí usually heavier).\n",
    "2. Car Speed & Travel Time ‚Üí negative covariance (faster ‚Üí less time).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Correlation (Related, but not cause-effect)\n",
    "- Similar to covariance, but **standardized** (between ‚Äì1 and +1).  \n",
    "- Shows **strength + direction** of relationship.  \n",
    "\n",
    "**Examples:**  \n",
    "- Study hours & exam marks ‚Üí **+0.9** (strong positive).  \n",
    "- Shoe size & intelligence ‚Üí ‚âà **0** (no relation).  \n",
    "- **Ice Cream Sales & Drowning Cases:**<br>\n",
    "In summer, both increase.<br>\n",
    "Correlated, but hot weather is the hidden factor.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Causation (Direct cause-effect)\n",
    "- Causation means one variable directly influences or produces a change in another.\n",
    "- In other words: X causes Y.\n",
    "- To prove causation, we usually need controlled experiments (not just data observation).\n",
    "**Example:**<br>\n",
    "1. Smoking ‚Üí causes lung cancer (proven by medical studies).\n",
    "2. Exercise ‚Üí reduces risk of heart disease.\n",
    "\n",
    "### üîπ Correlation vs Causation:\n",
    "- **Correlation** = \"They move together, but reason may be different (or unknown).\"\n",
    "- **Causation** = \"One actually makes the other happen.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Bias\n",
    "- Error caused by wrong or overly simple assumptions in a model.\n",
    "- If a model is too simple, it cannot capture the real patterns in the data.\n",
    "- Leads to underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Variance Machine Learning (Bias‚ÄìVariance Tradeoff)\n",
    "- Error caused by too much sensitivity to training data.\n",
    "- If the model is too complex, it starts memorizing noise instead of general patterns.\n",
    "- Leads to overfitting.\n",
    "\n",
    "**Example:** A model that fits every training point exactly, but fails on new data.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Underfitting\n",
    "- Model is too simple ‚Üí cannot learn patterns in the data.\n",
    "- Both training error and test error are high.\n",
    "**Example:** Predicting house prices using only the number of rooms (ignoring location, area, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Overfitting\n",
    "- Model is too complex ‚Üí learns both patterns and noise from training data.\n",
    "- Training error is very low but test error is high.\n",
    "**Example:** A decision tree with too many branches that perfectly classifies training data but fails on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Collinearity \n",
    "- Collinearity happens when two or more independent variables in a dataset are highly correlated with each other, meaning they contain overlapping information about the outcome.<br><br>\n",
    "**Example**<br>\n",
    "Features: House Size (sq ft) and Number of Rooms.<br>\n",
    "Both are highly correlated (bigger houses ‚Üí more rooms).<br>\n",
    "Model gets confused which feature to rely on ‚Üí collinearity.<br>\n",
    "* ‚ÄúWhich variable should I give more importance to?‚Äù\n",
    "* ‚ÄúIs the price changing because of size or because of rooms?‚Äù"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
